---
title: 概率导论 第九章 经典统计推断
date: 2021-02-14 20:00:08
tags: statistic inference
category: 
- statistics
- probablity
mathjax: True

---

在经典统计推断中，认为未知参数$\theta$是确定的未知量。观测值$X$是随机的，根据$\theta$取值的不同，服从$p_X(x;\theta)$或$f_X(x;\theta)$。因此我们是在多个概率模型中选择一个对参数$\theta$进行估计。

![image-20200305215707261](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200305215708-298018.png)

### 1. 经典参数估计

#### 1.1 估计量的性质

给定观测$X=(X_1,\dots,X_n)$, 估计量是指形式为$\hat \Theta=g(X)$的随机变量。由于$X$的分布依赖于$\theta$, 因而$\hat \Theta$的分布也一样依赖$\theta$。估计量$\hat \Theta$的取值称为估计量。

![image-20200305220843125](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200305220843-425800.png)

无偏的估计量是相对于估计量的偏差而言，我的理解是无偏估计代表了对未知参数的最好估计。这个估计量会有误差，但是期望值和未知参数相同。换句话说，我们能够达到最好的对未知量的估计就是估计量的期望值与未知量相同。

#### 1.2 最大似然估计

![image-20200305222621837](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200305222623-595176.png)

在实际的使用中，我们常常假设观测向量$X=(X_1,\dots,X_n)$是互相独立的，因此我们可以将公式转换一下形式

$p_X(x_1,\dots,x_n;\theta)=\prod_{i=1}^n p_{X_i}(x_i;\theta)$ 然后进行对数处理，将乘积转为求和，得到对数似然函数:

$ln\ p_X(x_1,\dots,x_n;\theta)=ln\ \prod_{i=1}^n p_{X_i}(x_i;\theta) = ln\ \sum_{i=1}^n p_{X_i}(x_i;\theta)$. 然后我们对$\theta$求导，就可以得到$\theta$的最大似然估计了。

对连续性变量进行类似处理，可以得到连续变量的对数似然函数 $ln\ f_X(x_1,\dots,x_n;\theta)=ln\ \prod_{i=1}^n f_{X_i}(x_i;\theta) = ln\ \sum_{i=1}^n f_{X_i}(x_i;\theta)$

此处对于术语“似然“需要一些的解释．对于已知$X  $的观测值$ x$, $P_X(x; \theta)$不是未知参数等于$\theta$的概率．事实上，这是当参数取值为 $\theta$ 时， 观测值 $x $可能出现的概率。因此为取定 $\theta $的估计值时, 我们会问这样的问题：基于已知的观测， $\theta $取什么值可使观察值最可能出现呢？这就是术语“似然”的本意．

回忆在贝叶斯最大后验概率估计中 ，估计量的选择是使表达式$P\Theta(\theta)P_{X|\Theta}(x |\theta) $取遍 $\theta  $达到最大, 其中$ P\Theta (\theta)$  是包含一 个未知离散参数$ \theta$的先验分布列。因而若将$ p_X(x; \theta) $看作条件概率密度函数，可将最大似然估计解释为具有均匀先验的最大后验概率估计．所谓均匀先验分布列是指对于所有$\theta$都具有一样的先验概率，也即没有任何信息的先验分布列 同样地对于连续的取值有界的$ \theta$, 可将最大似然估计解释为具有均匀先验密度的最大后验概率估计，其均匀先验密度为$f_\Theta(\theta) =  c$.

最大似然估计有一些明显的性质．比如说，它遵循不变原理：如果$\hat \Theta_n$是 $\theta$  的最大 似然估计，那么  对于任意关于 $\theta$ 一一映射的函数$h,\ \xi=h(\theta)$ 的最大似然估计是 $h(\Theta_n)$对于独立同分布的观测，在一些适合的假设条件下，最大似然估计量是相合的。

另一个有趣的性质是当$\theta$ 是标量参数的时候，在某些合适的条件下，最大似然估计量具有渐近正态性质 特别地，可以看见    $(\hat\Theta_n-\theta)/\sigma(\hat \Theta_n)$  的分布接近标准正态分布，其中  $ \sigma^2(\hat\Theta_n)$ 是$\hat \Theta_n$ 的 方差．因此如果我们还能够估计 $\sigma(\hat \Theta_n)$ , 就能进一步得到基于正态近似的误差方差估计．当$\theta$ 是向量参数 ， 针对每个分量可以得到类似的结论

![image-20200305224758957](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200305224759-387907.png)

#### 1.3 随机变量均值和方差的估计

![image-20200309215031466](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200309215031-165179.png)

样本均值的估计量易得，样本均值估计量的均方误差等于方差：

$E[(M_n - E[M_n])^2]=E[(M_n - \theta)^2]=Var(M_n)=Var(\frac{X_1+\dots+X_n}{n})=\frac{n\cdot Var(X_i)}{n^2}=\frac{v}{n}$

$Var(X_1\dots X_n)=nVar(X_i)$ 因为$X_1,\dots,X_n$是独立同分布。

然后我们可以求得方差的估计量：

一个自然的选择是 $\overset{-}{S_n^2}=\frac{1}{n}\sum_{i=1}^n (X_i -M_n)^2$

![image-20200309221022368](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200309221023-671954.png)

#### 1.4 置信区间

考虑未知参数$\theta$的一个估计量$\hat \Theta_n$. 除了估计所得的数值，我们还想建立一个所谓的置信区间。粗糙的说，这个区间以某个很高的概率包含参数$\theta$的真值。

首先我们固定一个希望达到的置信水平$1-\alpha$，其中$\alpha$往往是一个很小的数，然后我们用一个略小($\hat\Theta_n^-$)和略大的估计量($\hat\Theta_n^+$)代替点估计量$\hat \Theta_n$,于是$P_\theta(\hat\Theta_n^- \leq \hat\Theta \leq \hat\Theta_n^+) \geq 1-\alpha$.

![image-20200309221845882](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200309221847-720032.png)

通常情况下， 置信区间是包含估计量 $\hat\Theta_n $的区间更进一步， 在许多符合要求的置信区间中， 我们喜欢长度最短的．但是 这并不容易找到 因为误差$\hat\Theta_n -  \theta $ 的分布或者是未知的 或者是依赖于$\theta$ 的. 所幸在很多重要的模型中， $\hat\Theta_n -\theta$ 的分布是渐近正态无偏的．这就是说随机变量$\frac{\hat\Theta_n-\theta}{\sqrt{var_\theta(\hat\Theta_n)}}$的概率分布函数在$n$ 增加的时候趋于标准正态概率分布函数对（对于$ \theta$ 所有可能的取值).

![image-20200309222355630](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200309222359-969226.png)

#### 1.5 基于方差近似估计量的置信区间

![image-20200309223420220](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200309223420-457910.png)

![image-20200309223452631](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200309223454-529502.png)

随机变量$T_n$同样是正态变换，只是这里变换后并不是正态分布，而是符合自由度$n-1$的$t$分布。当$n$越大，$t$分布越接近于正态分布。

### 2. 线性回归

线性回归对感兴趣的两个或更多个变量之间的关系建立模型，又最小二乘法完成，不用考虑任何概率上的解释。

![image-20200309224629470](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200309224633-277475.png)

在于实际问题中，关于线性模型的假定未必是正确的，因此我们必须经过鉴定确认我们所处理的模型是一个线性模型，再应用最小二乘法去找出这个线性模型。

![image-20200309224939456](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200309224939-709074.png)

推导过程：

已知残差平方和公式：$F(\theta_0, \theta_1)=\sum_{i=1}^{n}（y_i-\hat y_i)^2 = \sum_{i=1}^n(y_i - \theta_0 -\theta_1 x_i)^2$

对$\theta_0,\ \theta_1$分别求导，可得

$\frac{\partial F}{\partial \theta_0}=-2\sum_{i=1}^{n}(y_i-\theta_0-x_i\cdot\theta_1)$

$\frac{\partial F}{\partial \theta_1} = -2\sum_{i=1}^n(y_i-\theta_0-x_i\theta_1)(x_i)=-2\sum_{i=1}^n(x_iy_i-\theta_0x_i-x_i^2\theta_1)$

令导数为$0$，可得

$\begin{aligned} & -2\sum_{i=1}^{n}(y_i-\theta_0-x_i\cdot\theta_1)=0 \\& \Rightarrow \sum_{i=1}^{n}y_i-\sum_i^n \theta_0-\sum_i^n x_i\cdot\theta_1 =0 \\&\Rightarrow n\theta_0 = ny_i - nx_i\theta_1 \\& \Rightarrow \theta_0=\overset{-}y-\overset{-}x\theta_1 \end{aligned}$

这里 $\overset{-}y = \frac{n y_i}{n};\ \overset{-}{x}=\frac{n x_i}{n}$

$\begin{aligned} & -2\sum_{i=1}^n(y_i-\theta_0-x_i\theta_1)(x_i)=0 \\& \Rightarrow \sum_{i=1}^n(x_i y_i - \theta_0 x_i - x_i^2 \theta_1) = 0 \\& \Rightarrow \sum_{i=1}^n(x_i y_i - (\overset{-}y-\overset{-}x\theta_1)x_i-x_i^2\theta_1)  =0 \\& \Rightarrow \sum_{i=1}^n (x_i y_i-\overset{-}yx_i + \overline x x_i\theta_1 - x_i^2 \theta_1) =0 \\&\Rightarrow \sum_{i=1}^n(x_i(y_i- \overline y)-x_i\theta_1(x_i-\overline x)) = 0 \\& \Rightarrow \sum_{i=1}^nx_i(y_i-\overline y) - \sum_{i=1}^nx_i\theta_1(x_i-\overline x) = 0  \\& \Rightarrow \theta_1 = \frac{\sum_{i=1}^nx_i(y_i-\overline y)}{\sum_{i=1}^nx_i(x_i-\overline x)} = \frac{\sum_{i=1}^n(x_iy_i-x_i\overline y)}{\sum_{i=1}^n (x_i^2-x_i\overline x)} \end{aligned}$

运用求和性质，我们得出分子和分母的另一个形式

$\begin{aligned} \sum_{i=1}^n (x_i - \overline x)(y_i - \overline y) & = \sum_{i=1}^n  (x_i y_i - x_i\overline y - y_i \overline x + \overline x \overline y)\\& = \sum_{i=1}^n x_i y_i - \sum_{i=1}^n x_i\overline y - \sum_{i=1}^n y_i\overline x + \sum_{i=1}^n \overline x \overline y \\& = \sum_{i=1}^n x_iy_i - n\overline x\overline y - n\overline x \overline y + n\overline x \overline y \\& =\sum_{i=1}^n x_iy_i - n\overline x \overline y \\& = \sum_{i=1}^n(x_iy_i - x_i\overline y) \end{aligned} $

$\begin{aligned} \sum_{i=1}^n (x_i-\overline x)^2 & = \sum_{i=1}^n (x_i^2 - 2x_i\overline x +{\overline x}^2) \\& = \sum_{i=1}^nx_i^2 - 2\sum_{i=1}^nx_i\overline x+\sum_{i=1}^n{\overline x}^2 \\&=\sum_{i=1}^n x_i^2 - 2n\overline x\overline x + n{\overline x}^2 \\& =\sum_{i=1}^n x_i^2 - n\overline x^2 \\&= \sum_{i=1}^n (x_i^2 - x_i\overline x)\end{aligned}$

因此

$\theta_1 = \frac{\sum_{i=1}^n (x_i - \overline x)(y_i - \overline y)}{\sum_{i=1}^n (x_i-\overline x)^2}$

#### 2.1 最小二乘公式的合理性

基于概率论的考虑，我们可以从三个角度来验证最小二乘公式的合理性

##### a. 最大似然（线性模型，正态噪声）

​    假定$x_i$是给定的数（不是随机变量），$y_i$是随机变量$Y_i$的实现，$Y_i$的模型为$Y_i=\theta_0 + \theta_1x_i+W_i,\  \ \ i = 1,\dots,m$

​    其中$W_i$是均值为零，方差为$\sigma^2$的正态独立同分布随机变量。因而$Y_i$也是独立的正态随机变量，均值为$\theta_0+\theta_1 x_i$,方差为$\sigma^2$.似然函数的形式为

$f_Y{(y;\theta)}=\prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma}}exp\{-\frac{(y_i-\theta_0-\theta_1x_i)^2}{2\sigma^2}\}$.

似然函数达到最大等于表达式中的指数部分达到最大，也就是残差平方和最小。因此，基于最小二乘法得到的$\theta_0$和$\theta_1$可以看作是满足正态独立同分布条件下对$y$的最大似然估计量。同时，**当$Y$与$X$满足这个条件时（正态独立同分布和线性关系），最小二乘法得到的估计量也就是无偏估计量**。

##### b. 近似贝叶斯线性最小均方估计（在可能的非线性模型中）

​    假设$x_i$和$y_i$时$X$和$Y$的实现，$(X_i,Y_i)$是独立同分布，但是联合分布$(X_i,Y_i)$是未知的。这时我们知道$(X_0,Y_0)$是符合$(X_i,Y_i)$分布的独立数对。因此我们可以运用第八章贝叶斯线性最小均方估计来求$\theta_0$和$\theta_1$。在第八章中，我们假设$(X_0, Y_0)$都是已知的，因此我们只是要求$\theta_0$和$\theta_1$来求$\hat Y  = \theta_0 + \theta_1X_0$。在第八章中，我们已经求得$\theta_1=\frac{cov(X,Y)}{var(X)};\ \theta_0 = E[Y_0] - \theta_1E[X_0]$. 由此得出$Y$的线性最小均方估计量: $E[Y_0] + \frac{cov(X_0,Y_0)}{var(X_0)}(X_0-E[X_0])$.

​    因为不知道$(X_0,Y_0)$的联合分布，因此我们样本方差和均值代替真实方差和分布：

​    $E[X_0]=\overline x$;

​    $E[Y_0] = \overline y; $

​    $cov(X_0, Y_0) = \frac{\sum_{i=1}^n(x_i-\overline x)(y_i -\overline y)}{n};$

​    $ var(X_0)=\frac{\sum_{i=1}^n(x_i-\overline x)^2}{n}$

​    这里就和我们最小二乘法得出的$y$的估计量一模一样了。而且这里不需要线性模型正确性的假设。

##### c. 近似贝叶斯最小均方估计（线性模型）

​    假设数据对$(X_i,Y_i)$独立同分布，且满足以下的表达式：

​    $Y_i = \theta_0 + \theta_1 X_i +W_i$

​    $W_i$是独立同分布的零均值噪声项，与$X_i$独立。根据条件期望的最小均方误差性质，$E[Y_0|X_0]$使最小均方误差的期望$E[E[Y_0] - g(X_0)]^2$达到最小。

​    因此真实的$\theta_0, \theta_1$使$E[(Y_0-\hat \theta_0 - \hat \theta_1 X_0)^2]$达到最小。由弱大数定律，这个表达式是当$n\rarr \infty$时$\frac{1}{n}\sum_{i=1}^n(Y_i-\hat \theta_0 - \hat\theta_1 X_i)^2$的极限。这说明$E[(Y_0-\hat \theta_0 - \hat \theta_1 X_0)^2]$达到最小是对$\frac{1}{n}\sum_{i=1}^n(Y_i-\hat \theta_0 - \hat\theta_1 X_i)^2$的最好近似，也就是对最小二乘法的最好近似，因为两个表达式一模一样。

​    **这里需要注意最小均方误差也是一个随机量，因此我们可以运用弱大数定律，在有足够多的样本后，最小均方误差的样本均值也就收敛于其实际期望。**

#### 2.2 贝叶斯线性回归

​    ![image-20200312223501439](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200312223503-20245.png)

​    推导过程：

​        基于$\Theta_0,\Theta_1,W_1,\dots,W_n$都是正态随机变量的假设，现在可以利用最大后验概率方法来推导贝叶斯估计量。在所有$\theta_0$和$\theta_1$中让后验概率密度函数$f_{\Theta|Y}(\theta_0,\theta_1|y_1,\dots,y_n)$最大。根据贝叶斯准则，后验概率密度函数是

​    $f_\Theta(\theta_0,\theta_1)f_{Y|\Theta}(y_1,\dots,y_n|\theta_0,\theta_1)$

除以一个和$(\theta_0,\theta_1)$无关的归一化常数。根据正态性假设，表达式写成

​    $c\cdot exp\{-\frac{\theta_0^2}{2\sigma_0^2}\}\cdot exp\{-\frac{\theta_1^2}{2\sigma_1^2}\}\cdot\prod_{i=1}^n exp\{-\frac{(y_i-\theta_0-x_i\theta_1)^2}{2\sigma^2}\}$

因为$Y_i = \theta_0 + \theta_1x_i+W_i \Rightarrow E[y_i] = \theta_0 + \theta_1 x_i$。在这里需要明确的是$Y_i$是对于给定的$x_i$的一个正态随机变量，并不是一个确定的值，其均值为$\theta_0 + \theta_1 x_i$, 方差为$\sigma^2 $，因此每一个单独项都可以转化为正态分布。

因此我们可以忽略常数，专注让上述表达式最大，其实就是最小化下列表达式

​    $\frac{\theta_0^2}{2\sigma_0^2} +\frac{\theta_1^2}{2\sigma_1^2}+ \frac{(y_i-\theta_0-x_i\theta_1)^2}{2\sigma^2}$

对$\theta_0, \theta_0$求导并取零，则可以求出上述解。

**重点**：

- 如果$\sigma_1^2$和 $\sigma_0^2$ 相比$\sigma^2$很大，则得到$\hat\theta_0 \approx 0; \hat\theta_1 \approx 0$。这种情况是噪声很大，观测基本被忽略，因而估计和先验均值（假设为零）是一样的。
- 如果先验方差$\sigma_0^2$ 和$\sigma_1^2$增加到无穷大，那么不存在任何关于$\Theta_0$和$\Theta_1$的有用的先验性息。在这种情况下，极大后验概率估计和$\sigma^2$不相干，其结果就和之前推导的经典线性回归公式一样。

  - 为简单起见假设 $\overline x$ =   0.  估计 $\Theta_1$时, 观测 $Y_i$ 的取值 $y_i$  的权重和其相关$x_i$的值是成比例的. 这可以从直观上来解释： 当 $x_i $  很大， $Y_i$中 $\Theta_1 x_i$ 的贡献就相对大，从而$Y_i$ 含有关于 $\Theta_1$ 有用的信息 反之， $x_i$ 为 0,  观测 $Y_i$  和
    $\Theta_1$ 独立，进而可以被忽略.
  - 估计$\theta_0$和$\theta_1$是 $Y_i$ 的线性函数，而不是 $x_i$   的．然而要记得， $x_i$ 是外生的非随机的数， 而 $y_i $ 是随机变量$Y_i$的观测值. 因而最大后验概率估计量$\hat\Theta_0$ 和 $\hat\Theta_1$是线性的 再看我们的正态性假设， 这些估计量同时又是贝叶斯线性最小均方估计量和最小均方估计量。

#### 2.3 多元线性回归

​    之前讨论的情况都是只含一个变量，被称为一元回归。在大多数情况时，我们都会处理多个自变量对一个应变量的解释，也就是多元线性回归。

​    在概念上，多元线性回归和一元回归没有差别，除了公式的复杂度会因为自变量的数量相应地提高。

​    同时，我们也可以引入高阶项, 因为我们要记住**线性关系描述的是未知参数$\theta_j$和观测的随机变量$Y_i$之间的线性关系**。

​    多元线性回归的一般形式： $y\approx\theta_0 + \sum_{i=1}^m\theta_j h_j(x)$

​    运用最小二乘法，求解使下面表达式最小的$\theta_0,\dots \theta_m$:  $\sum_{i=1}^n(y_i-\theta_0-\sum_{j=1}^m\theta_j h_j(x_i))^2$

#### 2.4 非线性回归

   如果关于未知参数的模型结构是非线性的，可将线性回归方法推广到非线性的情况。

​    假设变量$x$和$y$关系是： $y\approx h(x;\theta)$, 其中$h$是给定的函数，$\theta$是待估参数，对于已知的数据对$(x_i, y_i), i = 1,\dots,n$, 寻找$\theta$使得残差平方和$\sum_{i=1}^n(y_i-h(x_i;\theta))^2$ 达到最小。

​    非线性最小二乘估计源自参数$\theta$得最大似然估计。假定数据$y_i$来自下列模型：

​    $Y_i = h(x_i;\theta) + W_i, \ i = 1,\dots,n,$

​    其中$\theta$为未知得回归模型得参数，$W_i$是独立同分布得零均值得正态随机变量。这个模型得似然函数为

​    $f_Y(y;\theta) = \prod_{i=1}^n \frac{1}{\sqrt{{2\pi\sigma}}}exp\{-\frac{(y_i - h(x_i;\theta))^2}{2\sigma^2}\}$， $\sigma^2$是$W_i$的方差。

​    最大化这个似然函数就是最小化表达式中的指数项，也就是最小二乘估计量。   

​    **似然函数的推导与一元线性回归时的推导类似，但是核心都是假设$error$项，$W_i$，是独立同分布的噪声项，然后$Y_i$也符合正态分布。这些都是线性回归的重要假设条件。**

#### 2.5 实际中的考虑

​    a. 异方差性

​        在涉及正态误差的线性回归模型中，最小二乘估计要求模型中的误差项，也即$W_i(i=1,\dots,n)$的方差相同。但是现实中，有时很难满足这个条件，比如富人和穷人的收入方差就会很不一样，富人之间的收入方差会很大，因此$W_i$就会受到$X_i$的影响。这时，我们可以考虑使用加权最小二乘来估计参数，减少$X_i$对$W_i$的影响。

​    b. 非线性

​        很多时候，变量$x$的取值可以影响变量$y$的取值，但是这种影响可能是非线性的。因此对$x_i$的适当变换有时是必须的。

​    c. 多重共线性

​        假设现在有两个解释变量$x$和$z$来建模预测另一个变量$y$.如果$x$和$z$之间本身就有很强的关系，那么估计的过程可能无法可靠的区分两个解释变量各自对模型的影响。比如$y=2x+1$, 如果我们加入变量$z$, 我们就有$y = \theta_1x+\theta_2z+1$。通过最小二乘法，只要$\theta_1x+\theta_2z = 2x$, 任何$\theta_1,\ \theta_2$都是可能也是可以的。但是并没有办法可以准确的分摊两个强相关变量$x$和$z$在建立模型时对$y$的贡献。

​    d. 过度拟合

​        用大量的解释变量和相应的参数来建立多元回归可以得到良好的拟合效果，但这种方法是无效的。因为训练数据和实际数据很可能会有差别。一个重要的原则是，数据点的数量应该是待估参数个数的5倍，最好是10倍。

​    e. 因果关系   

​        并不能因为线性模型具有很好的拟合效果就认为变量$x$和$y$之间存在有因果关系，也许$x$是$y$的原因，也有可能相反，也有可能被其它因素共同影响而产生了类似的变化。因果关系的推导需要严谨的验证。

### 3. 简单假设验证

​    ![image-20200313224342145](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200313224343-282038.png)

​    ![image-20200313224406780](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200313224408-315356.png)

### 4. 显著性检验

#### 4.1 一般方法

![image-20200313225454813](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200313225456-70203.png)

#### 4.2 广义似然比和拟合优度检验

检验给定的分布列是否和观测数据保持一致，称为拟合优度检验。

广义似然比检验：

​    a. 通过最大似然来估计模型，比如选择在所有$\theta$中使得似然函数$p_X(x;\theta)$达到最大的参数向量$\hat \theta=(\hat \theta_1,\dots,\hat \theta_m)$.

​    b. 进行似然比检验，如果$\frac{p_X{(x;\hat\theta)}}{p_X(x;\theta^*)}$超过临界值$\xi$则拒绝$H_0$.一般取$\xi$等于显著水平$\alpha$.

![image-20200313230736821](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200313230737-173682.png)