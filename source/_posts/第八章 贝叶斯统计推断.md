---
title: 概率导论 第八章 贝叶斯统计推断
date: 2021-02-14 20:00:08
tags: bayesian
category: 
- statistics
- probablity
mathjax: True

---

统计领域的两大学派：

​    贝叶斯学派

​    频率学派

贝叶斯学派是将未知变量或模型看作为一个已知分布的随机变量，频率学派将其看作为一个待估计的未知量。

因此，贝叶斯学派都会有一个先验概率分布，然后通过已知信息，使用贝叶斯方法来求取未知变量的分布。

频率学派是将未知变量看作常数，对未知变量的估计就是从潜在的n个概率模型中选出最可能的那一个。

统计推断的应用主要有：模型推断和变量推断。其实常常很难区分二者，可以认为模型推断常常会是变量推断的一个工具，特别是在常见的回归分析中，模型的建立就是为了变量的推断。

![image-20200227223031629](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200227223032-348178.png)

### 1. 贝叶斯推断和后验分布

![image-20200227223206026](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200227223206-685117.png)

![image-20200227223219278](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200227223221-774047.png)

![image-20200227223238246](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200227223238-722914.png)

​    先验分布和后验分布属于同一个分布族的情况并不普遍，比较著名都有正态分布，伯努利分布和二项分布。正态分    布假设的普遍性可以说十分重要了，其特性对许多计算都非常重要。

​    贝叶斯计算也同样可以用于多个随机变量的估计，但是需要多维计算，在多个连续变量情况下需要计算多重积分十    分困难。

### 2. 点估计，假设检验，最大后验概率准则

最大后验概率（MAP): 给定观测值$x$,选择$\theta$的一个取值，记为$\hat \theta$，使得后验分布列$p_{\Theta|X}(\theta|x)$或后验分布连续函数$f_{\Theta|X}(\theta|x)$达到最大：

$$\hat \theta = arg \underset{\theta} {max} p_{\Theta|X}(\theta|x)\ (\Theta 离散) $$

$$\hat \theta=arg \underset {\theta} max f_{\Theta|X}(\theta|x)$$

当$\Theta$时离散变量时，最大后验概率准则有一条重要的最优性质：由于$\hat \theta$是$\Theta$最有可能的取值，它使对任意给定的$x$有最大的概率做出正确的决定。在考虑所有$x$的取值后，$\hat \theta$使得做出正确决策的概率最大，做出错误决策的概率最小（考虑所有的决策可能后）。

在贝叶斯准则下，有一条计算捷径去计算后验分布，对所有的$\theta$分母都一样（只受$x$影响），因此只需要计算$p_\Theta{\theta}p_{X|\Theta}(x|\theta)$并找到使之最大的$\theta$即可。这条捷径其实来自于全概率公式，比如在有两个决策可能下，其实就是计算在给定$x$下的条件概率并比较哪一个决策的条件概率更大，因此分母其实是一个常数（受$x$影响）。

![image-20200302231939374](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200302231940-97296.png)

![image-20200302231954679](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200302231956-215949.png)

   #### 2.1 点估计

​    点估计是一个数值，它表达了我们关于$\Theta$取值的最好猜测。估计值指的是在得到实际观察值$x$的基础下我们选择的$\hat \theta$的数值。$\hat \theta$的数值是由观测值$x$的某些函数$g$决定的，即$\hat \theta=g(x)$。随机变量$\hat \Theta = g(X)$也称为估计量，之所以说$\hat \Theta$是随机量是因为估计的结果由随机的观测值所决定。

​    利用不同的函数$g$可以构造不同的估计量，目前最流行的有两种估计量：

​        a. 最大后验概率估计量：观测到$x$，在所有的$\theta$中选$\hat \theta$ 使得后验分布达到最大。

​        b. 条件期望估计量：$\hat \theta = E[\Theta|X=x]$, 同时也可称为“最小均方(LMS)估计”，是估计量的误差达到最小。

​    最大后验概率估计量具有两个特性：

​        a. 如果 $\Theta$ 的后验分布关于（条件）均值对称并且是单峰的, 此时 $\Theta$ 的后验分布列（或后验密度函数）只有一个最大值，并且最大值在均值处取到．这时，最大后验概率估计量和条件期望估计量恰好一样, 例如后验分布保持为正态的情况。

​        b. 当$\Theta$ 是连续型变量， 有些时候最大后验概率统计量 $\Theta$ 的具体值可以通过分析的方法得到比如在对$\Theta$没有限制的情况下，将$f_{\Theta|X}(\theta|x)\ 或\ log f_{\Theta|X}(\theta|x)$导数取为0, 得到一个方程, 由方程解出 $\theta$ 即可,  但是在其他情况下，可能会需要通过数值计算的搜寻。

![image-20200302233951700](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200302233953-224862.png)

如果没有附加条件，点估计的准确性是没有多大保障的，两种估计量也可能相差甚远。

####    2.2 假设检验

​    ![image-20200302234407886](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200302234409-824394.png)

### 3. 贝叶斯最小均方估计

条件期望估计量具有使可能的均方误差达到最小的性质。

![image-20200303213422558](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200303213426-588821.png)

![image-20200303213509626](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200303213509-621163.png)

简单来说，就是未知量的在给定观测值后的条件期望其实使未知量的最优估计， 可以使估计量的误差最小。相比于最大后验概率估计量，估计量的确定都受到观测值的影响，但是条件期望估计量更稳定，能够最大可能减少均方误差。

   #### 3.1 估计误差的一些性质

​    $$\hat \Theta = E[\Theta|X],\tilde{\Theta}=\hat \Theta - \Theta$$

​    ![image-20200303220443411](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200303220444-346592.png)![image-20200303220508951](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200303220515-683593.png)

​    如果$X$是无信息的，$var(\hat \Theta) = 0, E[\Theta|X] = E[\Theta]$，$X$的取值不影响$\hat \Theta$的估计。

 ####   3.2 多次观测和多参数情况

​    我们可以将单变量下的最小均方估计延伸到多变量下的最小均方估计，但是常常会涉及到多维下的联合密度函数积    分，增加了计算的困难。因此我们常常是求解条件期望的近似值或是附加额外的条件来简化计算。最自然的考虑准    则$E[(\Theta_1 - \hat \Theta_1)^2 + \dots + E[(\Theta_m - \hat \Theta_m)^2]]$

​    我们的目的是求估计量$\hat \Theta_1,\dots,\hat \Theta_m$,使得上式在一切估计量中达到最小。这等价于$E[(\Theta_i - \hat\Theta_i)^2]$达到最小。因此，    多参数的估计问题本质是在处理多个单参数的估计问题：对于每个参数$\Theta_i$,其相应的最小均方估计为

​    $\hat \Theta_i=E[\Theta|X_1,\dots,X_n]$ 对所有的$i$成立。

### 4. 贝叶斯线性最小均方估计

因为条件期望估计量和最大后验概率估计量在实际情况中计算的复杂度非常高，我们可以使用观测值的线性组合来寻找统计量使估计值最小。

基于观测$X_1,\dots,X_n$的$\Theta$的线性估计量形式为$\hat \Theta = a_1 X_1 + \dots + a_n X_n +b$

给定$a_1,\dots,a_n,b$, 均方误差是$E[(\Theta-a_1 X_1 + \dots + a_n X_n +b)^2]$

#### 4.1 一次观测的线性最小均方估计

我们先从$n=1$的情况开始了解，再扩展到多个观测值的情况。

假设我们已经选好了$a$, 可以知道$b = E[\Theta - aX]=E[\Theta] - aE[X]$

将$a, b$代入到均方误差的公式中，可以得到$E[\Theta - a - E[\Theta] + a[X]]$, 因此我们将均方误差转化只有一个未知量$a$的函数，我们只需要求得使这个均方误差最小的$a$即可。

继续转化下均方误差的代表公式

$$\begin{aligned}E[\Theta - a - E[\Theta] - aE[X]] & = E[(\Theta - aX - E[\Theta - aX])^2] \\ &= Var(\Theta - aX) \\&= E[(\Theta -aX)^2] - E[\Theta - aX]^2 \\&= E[\Theta^2] - 2aE[X\Theta]+a^2E[X^2] - E[\Theta^2]+2aE[\Theta]E[X]-a^2E[X]^2 \\&=\sigma_\Theta^2+a^2X^2 - 2a(E[X\Theta] - E[X]E[\Theta]) \\& = \sigma_\Theta^2 + a^2X^2 - 2a \cdot cov(\Theta, X)  \end{aligned}$$

对$a$求导，可得$a = \frac{cov(\Theta,X)}{\sigma_X^2}=\frac{\rho\sigma_\Theta\sigma_X}{\sigma_X^2}=\rho\frac{\sigma_\Theta}{\sigma_X}$, 这里$\rho = \frac{cov(\Theta, X)}{\sigma_\Theta \sigma_X}$, 代表了$\Theta$和$X$的相关性, $cov(\Theta, X)=E[(\Theta - E[\Theta])(X -E[X])] = E[X\Theta] - E[X]E[\Theta]$。

所以根据$a$的选择，估计量$\hat \Theta$的均方估计误差是

$$\begin{aligned} var(\Theta - \hat \Theta) &= \sigma_\Theta^2 + a^2X^2 - 2a \cdot cov(\Theta, X) \\&= \sigma_\Theta^2 +\rho^2\frac{\sigma_\Theta^2}{\sigma_X^2}\sigma_X^2-2\rho\frac{\Sigma_\Theta}{\sigma_X}\rho\sigma_\Theta \sigma_X \\& = (1-\rho^2) \sigma_\Theta^2  \end{aligned}$$

将$a$, $b$代入公式$\Theta = aX+b$, 我们就可以得到线性最小均方估计：

$\begin{aligned}\hat \Theta & = aX + E[\Theta]-aE[X] \\& = E[\Theta] + a(X - E[X]) \\& = E[\Theta] + \rho\frac{\sigma_\Theta}{\sigma_X}(X-E[X]) \end{aligned}$

![image-20200304000038937](https://raw.githubusercontent.com/ydeng11/typora_pics/master/typora20200304000039-571627.png)

线性最小均方估计的公式只包括均值、方差以及$\Theta$ 与$X$间的协方差. 更进一步，它有个直观的解释, 为描述准确起见， 假设相关系数$\rho$是正的  估计量以$\Theta$ 的基本估计$E[\Theta]$ 为基础  通过 $X  -  E[X ] $的取值来调整.举例来说，当  X   比均值大，则 $X$ 与 $ \Theta$  之间相关系数告诉我们预期中的$\Theta$将大于它的均值．因此，估计量会是一个大于$E[\Theta]$ 的取值 . $\rho$的取值同样也会影响估计的质量．当$\rho$接近 1 的时候，两个随机变量高度相关，而均方估计误差接近$0$.

#### 4.2 多次观测和多参数情形

类似于条件期望估计量的处理方式，多参数情形下我们将问题转变为多个单参数的问题。

在假设多次观测独立且同分布时，我们可以假设$\Theta$服从均值为$\mu$,方差为$\sigma_0^2$的随机变量， $X_1,\dots,X_n$具有如下形式的多次观测 $X_i=\Theta + W_i$, 这里$W_i$时均值为0，方差为$\sigma_i^2$的随机变量。

因此目标函数为 $h(a_1,\dots,a_n,b)=E[(\Theta - a_1X_1-\dots-a_nX_n-b)^2]$, 为求其最小值，对$a$和$b$分别求偏导，可得：

$b=\frac{\mu/\sigma_0^2}{\sum_{i=0}^n 1/\sigma_i^2},\ a_j = \frac{1/\sigma_j^2}{\sum_{i=0}^n 1/\sigma_i^2},\ j=1,\dots,n$

#### 4.3 线性估计和正态模型

线性最小均方估计和最小均方估计具有不同的形式，往往线性最小均方估计次于最小均方估计（因为线性假设的缘故），但如果观测值的线性组合恰恰等于最小均方估计，则两个估计量重合。

这种情况发生的一个重要例子是： $\Theta$是一个正态随机 变量，观测值是$ X_i=\Theta+W_i$, 其中 $W_i$ 是独立零均值的正态噪声项同时与 $\Theta$ 独立。我们看到$\Theta$的后验分布是正态的, 其条件均值$E[\Theta|X_1,···, X_n］$是观 测

值的线性函数。因此最小均方估计量和线性最小均方估计量是重合的。

这个结果还可以进一步推广：  如果 $ \Theta, X_1,· ·· , X_n $都是一些独立正态随机变量的线性函数，那么最小均方估计和线性最小均方估计量是一致的．它们和最大后验概率统计量也是一致的，这是由于正态分布是单峰对称的．

上面的讨论提出了线性最小均方估计量的一种有趣的性质：将原模型进行改变，在保持均值、方差和协方差不变的情况下，假设牵涉到的随机变量都服从正态分布，在改变了的模型中得到的估计量（最大后验概率估计量、最小均方估计量和线性最小均方估计量都是相同的）恰好就是原模型中的线性最小均方估计量．因此，线性最小均方估计量有两方面的价值 ：一种是计算的简便（避免公式$E[\Theta|X ]$ 的复杂计算），另一种是模型的简化（用正态分布替代较难处理的分布).

**这个例子介绍了线性回归正态分布假设的由来，因为假设随机变量都服从正态分布的话，得到的线性最小均方估计量就是最优估计量了，和最大后验概率估计量，最小均方估计量都是相同的。**

#### 4.4 线性估计的变量选择

另一个线性估计的优点：观测值的映射变换不影响线性最小均方的估计，但是会影响最小均方的估计。然而有些时候$X_1,\dots,X_n$可能并不存在对$\Theta$的一个合理的估计量，比如$\Theta$时某分布的未知方差，$X_1,\dots,X_n$是这个分布的独立抽样的随机变量。这样，$X_1,\dots,X_n$的线性组合就无法找到对$\Theta$的合理估计。这时候，我们就可以通过**映射变换**改变变量来找到一个对$\Theta$的好的估计。当然这并不容易。

**在线性回归的事件中，变量的映射变换是一个常用的处理因变量的办法，上述其实说明了对变量进行变换的合理性和必要性。**